# Build ETL data pipelines with Shell, Airflow and Kafka 

In this final assignment module, you will apply your newly gained knowledge to explore two very exciting hands-on labs. “Creating ETL Data Pipelines using Apache Airflow” and “Creating Streaming Data Pipelines using Kafka”. 

You will explore building these ETL pipelines using real-world scenarios. You will extract, transform, and load data into a CSV file. You will also create a topic named “toll” in Apache Kafka, download and customize a streaming data consumer, as well as verifying that streaming data has been collected in the database table.

Objectives:
- Extract data from CSV, TSV, and fixed width files
- Transform extracted data
- Load transformed data into the staging area
- Submit, unpause, and monitor a DAG
- Create a topic in Kafka
- Download and customize a streaming data consumer
- Verify that streaming data has been collected in a database table
- Evaluate your peers’ code and output using the provided rubric and grading scheme

## 1.0: Build an ETL Pipeline using Airflow 

steps:
- Extract data from a csv file 
- extract data from a tsv file 
- extract data from a fixed width file 
- transform the data 
- load the transformed data into the staging area

### 1.1: Prepare the lab environment 
#### 1.1.1: Start Apache Airflow   
<img src="https://imgur.com/gMNUbea.png">  

### 1.2: Create a DAG
#### 1.2.1: Define DAG arguments  
<img src="https://imgur.com/sx8Eray.png">   

#### 1.2.2: Define the DAG  
<img src="https://imgur.com/uFLq0ys.png">   

#### 1.2.3: Create a ask to unzip data   
<img src="https://imgur.com/8u3UFck.png">    

#### 1.2.4: Create a task to extract data from csv file 
<img src="https://imgur.com/rsjydO7.png">  

#### 1.2.5: Create a task to extract data from tsv file 
<img src="https://imgur.com/7tiYp3F.png">  

#### 1.2.6: Create a task to extract data from fixed width file  
<img src="https://imgur.com/awSt9Q9.png">  

#### 1.2.7: Create a task to consolidate data extracted from previous tasks
<img src="https://imgur.com/Vtwmd6q.png">  

#### 1.2.8: Transform and load the data
<img src="https://imgur.com/I8IvmQ5.png">   

#### 1.2.9: Define the task pipeline 
<img src="https://imgur.com/sFi2JHS.png">

### 1.3: Getting the DAG operational 
#### 1.3.1: Submit the DAG 
<img src="https://imgur.com/60Ojidc.png">  

#### 1.3.2: Unpause the DAG
#### 1.3.3: Monitor the DAG  
<img src="https://imgur.com/cHGC9Xo.png">

##

## 2.0:Build a Streaming Pipeline using Kafka 

Tools/Technologies: Apache Kafka , MySQL server, Python, CLI in Linux   

As a data engineer at data analytics consulting company, I'm assigned to a project that aims to de-congest the national highways by analyzing the road traffic data from different toll plazas. As a vehicle passes a toll plaza, the vehicle's data like ```vehicle_id```, ```vehicle_type```, ```toll_plaza_id``` and timestamp are streamed to Kafka. My job is to create a data pipeline that collects the streaming data and loads it into a database. 

Steps:
- start a MySQL database server
- create a table to hold the toll data 
- start Kafka server 
- install kafka python driver 
- install the MySQL python driver 
- create a topic named toll in kafka 
- download streaming data generator program 
- customize the generator program to steam to toll topic 
- download and customize streaming data consumer 
- customize the consumer program to write into a MySQL database table 
- verify that streamed data is being collected in the db table

### 2.1: Prepare the lab environment 
#### Step 1:  Download Kafka 
<img src ="https://imgur.com/IOvtC5e.png">   

#### Step 2: Extract Kafka  
<img src ="https://imgur.com/oMCeGFO.png">  

#### Step 3: Start MySQL server  
<img src ="https://imgur.com/ngbqKaG.png">   

#### Step 4: Connect to the MySQL server  
<img src ="https://imgur.com/010hYzm.png">  

#### Step 5: Create a database named ```tolldata```   
<img src ="https://imgur.com/jNkj75Q.png">  

#### Step 6: Create a table named ```livetolldata``` with the schema to store the data generated by the traffic simulator   
<img src ="https://imgur.com/1QT9QxW.png">

#### Step 7: Install the pyhon module ```kafka-python``` using the pip command   
<img src ="https://imgur.com/zk11Q8q.png"> 

#### Step 8: Install the python module ```mysql-connector-python``` using the pip command   
it helps to interact with mysql server    
<img src ="https://imgur.com/o7iXglV.png">


### 2.2: Start Kafka
#### 2.2.1: Start Zookeeper   
<img src ="https://imgur.com/fNWnjUj.png">

#### 2.2.2: Start Kafka server    
<img src ="https://imgur.com/7ErKF54.png"> 

#### 2.2.3: Create a topic named ```toll```   
<img src ="https://imgur.com/OR6qqj3.png"> 

#### 2.2.4: Configure the Toll Traffic Simulator   
Build a toll traffic simulator to ingest real-time into database using kafka consumer                     
<img src ="https://imgur.com/n3YSBZG.png">

#### 2.2.5: Run the Toll Traffic Simulator   
<img src ="https://imgur.com/mKZgFuF.png"> 

#### 2.2.6: Configure ```streaming_data_reader.py``   
To load streaming data , toll traffic simulator, into MySQL database   

<img src ="https://imgur.com/xdHMp1w.png">   
<img src ="https://imgur.com/oioUXet.png"> 

#### 2.2.7: Run file ```streaming_data_reader.py``` python script 
<img src ="https://imgur.com/zZ9A7Ee.png">

#### 2.2.8: Health Check of the Streaming Data Pipeline by querying from MySQL database   
<img src ="https://imgur.com/TWoUddJ.png">  
