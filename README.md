# ETL-Data-Pipeline-with-Shell-Airflow-Kafka

In this final assignment module, you will apply your newly gained knowledge to explore two very exciting hands-on labs. “Creating ETL Data Pipelines using Apache Airflow” and “Creating Streaming Data Pipelines using Kafka”. 

You will explore building these ETL pipelines using real-world scenarios. You will extract, transform, and load data into a CSV file. You will also create a topic named “toll” in Apache Kafka, download and customize a streaming data consumer, as well as verifying that streaming data has been collected in the database table.

Objectives:
Extract data from CSV, TSV, and fixed width files
Transform extracted data
Load transformed data into the staging area.
Submit, unpause, and monitor a DAG.
Create a topic in Kafka
Download and customize a streaming data consumer
Verify that streaming data has been collected in a database table
Evaluate your peers’ code and output using the provided rubric and grading scheme.

## 1.0: Build an ETL Pipeline using Airflow 
### 1.2: Prepare the lab environment 
### 1.3: Create a DAG 
### 1.4: Getting the DAG operational 



## 2.0:Build a Streaming ETL Pipeline using Kafka 

steps:
- start a MySQL database server
- create a table to hold the toll data 
- start Kafka server 
- install kafka python driver 
- install the MySQL python driver 
- create a topic named toll in kafka 
- download streaming data generator program 
- customize the generator program to steam to toll topic 
- download and customize streaming data consumer 
- customize the consumer program to write into a MySQL database table 
- verify that streamed data is being collected in the db table

### 2.1: Prepare the lab environment 
### 2.2: Start Kafka 
