# ETL-Data-Pipeline-with-Shell-Airflow-Kafka

In this final assignment module, you will apply your newly gained knowledge to explore two very exciting hands-on labs. “Creating ETL Data Pipelines using Apache Airflow” and “Creating Streaming Data Pipelines using Kafka”. 

You will explore building these ETL pipelines using real-world scenarios. You will extract, transform, and load data into a CSV file. You will also create a topic named “toll” in Apache Kafka, download and customize a streaming data consumer, as well as verifying that streaming data has been collected in the database table.

Objectives:
- Extract data from CSV, TSV, and fixed width files
- Transform extracted data
- Load transformed data into the staging area
- Submit, unpause, and monitor a DAG
- Create a topic in Kafka
- Download and customize a streaming data consumer
- Verify that streaming data has been collected in a database table
- Evaluate your peers’ code and output using the provided rubric and grading scheme

## 1.0: Build an ETL Pipeline using Airflow 

steps:
- Extract data from a csv file 
- extract data from a tsv file 
- extract data from a fixed width file 
- transform the data 
- load the transformed data into the staging area

### 1.1: Prepare the lab environment 
#### 1.1.1: Start Apache Airflow
<img src="https://imgur.com/gMNUbea.png">

### 1.2: Create a DAG
#### 1.2.1: Define DAG arguments
<img src="https://imgur.com/sx8Eray.png">

#### 1.2.2: Define the DAG 
<img src="https://imgur.com/uFLq0ys.png"> 

#### 1.2.3: Create a task to unzip data 
<img src="https://imgur.com/8u3UFck.png">  

#### 1.2.4: Create a task to extract data from csv file 
<img src="https://imgur.com/rsjydO7.png">  

#### 1.2.5: Create a task to extract data from tsv file 
<img src="https://imgur.com/7tiYp3F.png">  

#### 1.2.6: Create a task to extract data from fixed width file  
<img src="https://imgur.com/awSt9Q9.png">  

#### 1.2.7: Create a task to consolidate data extracted from previous tasks
<img src="https://imgur.com/Vtwmd6q.png">  

#### 1.2.8: Transform and load the data
<img src="https://imgur.com/I8IvmQ5.png">   

#### 1.2.9: Define the task pipeline 
<img src="https://imgur.com/sFi2JHS.png">

### 1.3: Getting the DAG operational 
#### 1.3.1: Submit the DAG 
<img src="https://imgur.com/60Ojidc.png">  

#### 1.3.2: Unpause the DAG
#### 1.3.3: Monitor the DAG  
<img src="https://imgur.com/cHGC9Xo.png">

##

## 2.0:Build a Streaming ETL Pipeline using Kafka 

Steps:
- start a MySQL database server
- create a table to hold the toll data 
- start Kafka server 
- install kafka python driver 
- install the MySQL python driver 
- create a topic named toll in kafka 
- download streaming data generator program 
- customize the generator program to steam to toll topic 
- download and customize streaming data consumer 
- customize the consumer program to write into a MySQL database table 
- verify that streamed data is being collected in the db table

### 2.1: Prepare the lab environment 
#### 2.1.1: Download Kafka 
<img src ="https://imgur.com/IOvtC5e.png">   

#### 2.1.2: Extract Kafka 
<img src ="https://imgur.com/oMCeGFO.png">  

#### 2.1.3: Start MySQL server 
<img src ="https://imgur.com/ngbqKaG.png">   

#### 2.1.4: Connect to the mysql server 
<img src ="https://imgur.com/010hYzm.png">  

#### 2.1.5: Create a database named ```tolldata```
<img src ="https://imgur.com/jNkj75Q.png">  

#### 2.1.6: Create a table named ```livetolldata``` with the schema to store the data generated by the traffic simulator
<img src ="https://imgur.com/1QT9QxW.png">
#### 2.1.7: Disconnect from MySQL server  
<img src =".png">

#### 2.1.8: Install the pyhon module ```kafka-python``` using the pip command 
<img src ="https://imgur.com/zk11Q8q.png"> 

#### 2.1.9: Install the python module ```mysql-connector-python``` using the pop command
<img src ="https://imgur.com/o7iXglV.png">


### 2.2: Start Kafka
#### 2.2.1: Start Zookeeper
<img src ="https://imgur.com/fNWnjUj.png">

#### 2.2.2: Start Kafka server 
<img src ="https://imgur.com/7ErKF54.png"> 

#### 2.2.3: Create a topic named ```toll```
<img src ="https://imgur.com/OR6qqj3.png"> 

#### 2.2.4: Download the Toll Traffic Simulator 
<img src =".png">

#### 2.2.5: Configure the Toll Traffic Simulator 
<img src ="https://imgur.com/n3YSBZG.png">

#### 2.2.6: Run the Toll Traffic Simulator 
<img src ="https://imgur.com/mKZgFuF.png"> 

#### 2.2.7: Configure ```streaming_data_reader.py```
<img src ="https://imgur.com/xdHMp1w.png"> 
<img src ="https://imgur.com/oioUXet.png"> 

#### 2.2.8: Run ```streaming_data_reader.py```  
<img src ="https://imgur.com/zZ9A7Ee.png">

#### 2.2.9: Health Check of the streaming data pipeline
<img src ="https://imgur.com/TWoUddJ.png">
